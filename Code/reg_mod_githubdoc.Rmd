---
title: "Regression Models (Motor Trend Project)"
author: "Nutan Sahoo"
date: "21 September 2017"
output: rmarkdown::github_document
  
geometry: margin=1in
---

```{r setup, include= FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE, error=FALSE, fig.path = "README_figs/README-")
```

## Executive Summary
The purpose of this project is to explore the impact of a set of variables like horsepower, transmission configuration, engine cylinder configuration, etc. on the mileage `mpg` (Miles per Gallon). In particular we have the following objectives:---

1. To find which one of Automatic or Manual Transmission is better for `mpg`.
2. To Quantify the `mpg` difference b/w auto and manual transmissions.
3. To come up with a model to predict the mileage of a given car.
4. To find how a 1000lbs increase in car's weight will change the fuel efficiency.


Firstly, I plotted boxplots of  `mpg` against various categorical variables to visualise how `mpg` changes with changing levels and also to examine the spread and mean of the different levels of the factor. We visualise a correlation matrix using `chart.Correlation` function in `PerformanceAnalytics` Package. The variables which had high cor. with the mpg were selected as regressor variables for our regression model. Out of all possible regression models which can be made from the combination of those regressors, we select the best model using `stepAIC` function from `MASS` package on the basis on Akaike's Information Criteria, Other criteria such as adjusted R^2^, Mean Square of Residuals, Generalised Variance Inflation factors were also taken into consideration.

*****************
## Exploratory Analysis 
We plot various box plots to visualise the distribution of `mpg` by various groups of the categorical variable.^[**All plots given in appendix**] 
In plot-1 **(see appendix for all plots)** we see that the `mpg` of cars with automatic transmission is much lower than manual. We can check if the difference b/w their mean values are statistically significant through a two-samples independent t-test.


```{r }
aggregate(mpg~am, data = mtcars, mean)
t.test(mpg~am, data=mtcars)

```


**p value <0.05**, we reject the null hypothesis that the true mean difference is equal to zero. Hence, the difference is statistically significant and the cars with automatic transmission have a lower `mpg` on an average. Plot-2 shows that generally, `mpg` of cars with `S` engine configuration is much higher. We can confirm this using a t-test as shown above. 

```{r, message=FALSE, warning=FALSE}
t.test(mpg~vs, data= mtcars)
##t = -4.6671, df = 22.716, p-value = 0.0001098
```

p value is 0.0001, we reject the null hypothesis that the true mean difference b/w cars with different engine configuration is 0. The difference is statistically significant. The configuration of the engine significantly affects the mileage.
By looking at plot-3 it is safe to assume, higher th number of cylinders in the car, lower is the `mpg`. Such definite conclusions cannot be drawn by looking at plot-4 & plot-5 but we can perform t-tests for it.

```{r mtcars}
head(mtcars,3)
dim(mtcars)
names(mtcars)
```

For buliding the model we will randomly split the mtcars data into training and test set using `split.sample` function in `caTools` package.^[Since, data set is very small, I have just taken the last five(i.e. 15%) rows in the test set].

```{r, message=FALSE, warning=FALSE, eval=FALSE }
#install.packages("caTools")
library(caTools)
set.seed(88)       #to fix the result that we get as we get different results everytime we run it

spl = sample(1:nrow(data), size=0.85* nrow(data))

train = data[spl,]

test = data[-spl,]
#this is the method for splitting the data set into training and test data when the outcome variable is
#continuous
```
```{r}
mtcars1<- mtcars[-(28:32), ] #the training set
test_data<- mtcars[28:32, ]  #test data set
test_data
```

**************

## Regression Analysis
We have graphically seen that Manual is better for `mpg`. Now we will qantify the difference between them. 
```{r, results='hide'}
mtcars1$cyl<- factor(mtcars1$cyl)
mtcars1$vs<- factor(mtcars1$vs)
mtcars1$am<- factor(mtcars1$am)
mtcars1$carb<- factor(mtcars1$carb)
mtcars1$gear<- factor(mtcars1$gear)

```

There are various methods for choosing a subset of variables for the best regression model which can explain variability in response variable well. There are 2 classes of algorithms for that:---

1. All possible regression approach 
2. sequential selection
    i) forward selection 
    ii) backward selection
    
    
Models are selected on the basis of adj. R^2^, MS~Res~, Mallow's statistic and/or AIC.
Here I will use the 'all possible regression approach' algorithm and choose the best model on the basis of AIC using `StepAIC` funcion. 

### Selecting the Best Model

```{r,  warning=FALSE, message=FALSE}
#install.packages("MASS")
library(MASS)
stepAIC(lm(mpg~., mtcars1), direction = "both") #direction can be forward, backward or both 

m1<- lm(mpg ~ disp + hp + drat + wt + vs + am + gear + carb, data=mtcars1)
#checking for multicollinearity using vif and clorrelation chart
#install.packages("car")
library(car)
vif(m1)
#install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(mtcars1[,c(1,3,4,5,6,7)], histogram =T)

```

StepAIC fn. will form models with all possible combinations of regressors. Then model is selected on the basis of AIC (Akaike's Information Criteria);

The variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated.  If no factors are correlated, the VIFs will all be 1. A VIF between 5 and 10 indicates high correlation that may be problematic. And if the VIF goes above 10, you can assume that the regression coefficients are poorly estimated due to multicollinearity.
Remove highly correlated predictors from the model.  If you have two or more factors with a high VIF, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared.  Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables.

We remove `disp` as it is highly correlated with all the other regressors along with other regressors which had high vif. We fit various other models and select the best of them:
```{r}
m2<- lm(mpg~ hp+drat+wt+am+carb-1, mtcars1)
extractAIC(m2)
summary(m2)$coef 
#the coef are not significant and vif is also somewhat high

```
Variables like `hp`(horsepower), `wt` are expected to be significant but are not. Hence, problem due to multicollinearity still persists in the model. Hence, we remove `wt`. By looking at the correlation chart, we see that most of the regressor var. are correlated to each other. so we pick a model with less no. of regressors.
```{r}
m9<-  lm(mpg~am+wt, mtcars1)
m6<- lm(mpg~am+wt+hp, mtcars1)
anova(m6,m9)   #it is significant. 
m7<- lm(mpg~am+wt+hp+cyl, mtcars1)
anova(m6,m7);  #not significant according to anova

summary(m6)
vif(m6) 
extractAIC(m6) #variance inflation factor and AIC look good.

```

According to our objectives our model must contain `wt` and `am`. Hence we start from there and perform anova test to check if the added regressor add any new, relevant information to the model. `m6` is significant amongst all and it's variance inflation factor and AIC are also low. Mileage at zero weight and horsepower doesn't make any sense, we subtract the intercept.
```{r}
#final model
m6<- lm(mpg~am+wt+hp-1, mtcars1)
summary(m6)
#all coef are significant.
confint(m6, level = 0.95)
```

Let's check how accurately it can predict the mileage of the car in the `test_data`.
```{r}
test_data$cyl<- factor(test_data$cyl)
test_data$vs<- factor(test_data$vs)
test_data$am<- factor(test_data$am)
test_data$carb<- factor(test_data$carb)
test_data$gear<- factor(test_data$gear)


predict(m6, test_data) ; test_data
```
```{r}
1-(abs(predict(m6, test_data)-test_data[1]))/test_data[1]

#comparing predictions through mean and standard deviation of accuracy
mean((1-(abs(predict(m6, test_data)-test_data[1]))/test_data[1])[,1]);mean((1-(abs(predict(m7,
test_data)-test_data[1]))/test_data[1])[,1])                                                                                    
                                                                              

sd((1-(abs(predict(m6, test_data)-test_data[1]))/test_data[1])[,1]);sd((1-(abs(predict(m7,
   test_data)-test_data[1]))/test_data[1])[,1])                                                                                    

```

Predictions from m6 and the actual mileage of the cars in test data are very close with an average accuracy of abput 89%; model m6 is parsimonious and interpretable too.

### Checking basic assumptions of regression
```{r}
res<- m6$residuals
shapiro.test(res)

#install.packages("ggfortify")
library(ggfortify)
autoplot(m6, label.size=4)
```

**p>0.05** hence we fail to reject the null hypothesis that residuals are normally distributed.
By looking at diagnostic plots, we can say that the residuals are homoscedastic. The scatter points in QQ Plot also seem to lie on the line, thereby confirming their normality. 
Basic assumptions of regression are met. 


We can finally, say that **compared to cars which had automatic transmission(0) we would expect mileage of cars with manual transmission(1) 2.6 miles per gallon more on average given values of other regressor variables remain same**. By looking at the coefficient of the weight variable in the final model, we can conclude that a 1000lbs increase in the weight a car will decrease the mileage by 2.234 miles per gallon.


**************

## Appendix
This section includes all the above mentioned Plots.

```{r , echo=TRUE, fig.height=5, fig.width=11, message=FALSE, warning=FALSE, fig.cap=" "}
layout(matrix(c(1,2,1,2),2,2, byrow = TRUE))
boxplot(mpg~am,data=mtcars,col=c("red", "turquoise"),xlab="transmission type",ylab="miles per gallon", 
        names= c("Automatic","Manual"),main="Plot-1")
boxplot(mpg~vs, data= mtcars, col=c(4,"cyan"), xlab="Engine Cylinder Configuration",
        ylab="Miles/Gallon", las=TRUE, names= c("V shape", "Straight Line Shape"), main="Plot-2")
```


```{r  , fig.height=5, fig.width=11, message=FALSE, warning=FALSE, fig.cap=""}
layout(matrix(c(1,2,1,2),2,2, byrow = T))
boxplot(mpg~cyl, data= mtcars, col=c("cyan",42,23), 
        ylab="Miles per Gallon", las=TRUE, main="Plot-3")
boxplot(mpg~gear, data= mtcars, col=c("cyan",42,23), xlab="No. of Gears",
        ylab="Miles per Gallon", las=T, main= "Plot-4")
```


```{r , fig.height=5, fig.width=6, message=FALSE, warning=FALSE, fig.cap=""}
boxplot(mpg~carb, data= mtcars, col=c("cyan",42,23,"green",600), xlab="Number of carburetors",
        ylab="Miles per Gallon", las=T, main="Plot-5")
```


